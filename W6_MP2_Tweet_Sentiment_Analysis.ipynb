{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0385a4d8",
   "metadata": {},
   "source": [
    "# Week 6, Mini Project 2, Tweet Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acf95358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a236cb8d",
   "metadata": {},
   "source": [
    "# 1. Download the Dataset\n",
    "Download the dataset from the following link:\n",
    "https://data.world/crowdflower/sentiment-analysis-in-text "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb8cb8e",
   "metadata": {},
   "source": [
    "# 2. Reading the Dataset\n",
    "Read and prepare your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a792677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>xoshayzers</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>wannamama</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>coolfunky</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>czareaquino</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>xkilljoyx</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>1753918954</td>\n",
       "      <td>neutral</td>\n",
       "      <td>showMe_Heaven</td>\n",
       "      <td>@JohnLloydTaylor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>1753919001</td>\n",
       "      <td>love</td>\n",
       "      <td>drapeaux</td>\n",
       "      <td>Happy Mothers Day  All my love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>1753919005</td>\n",
       "      <td>love</td>\n",
       "      <td>JenniRox</td>\n",
       "      <td>Happy Mother's Day to all the mommies out ther...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>1753919043</td>\n",
       "      <td>happiness</td>\n",
       "      <td>ipdaman1</td>\n",
       "      <td>@niariley WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>1753919049</td>\n",
       "      <td>love</td>\n",
       "      <td>Alpharalpha</td>\n",
       "      <td>@mopedronin bullet train from tokyo    the gf ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id   sentiment         author  \\\n",
       "0      1956967341       empty     xoshayzers   \n",
       "1      1956967666     sadness      wannamama   \n",
       "2      1956967696     sadness      coolfunky   \n",
       "3      1956967789  enthusiasm    czareaquino   \n",
       "4      1956968416     neutral      xkilljoyx   \n",
       "...           ...         ...            ...   \n",
       "39995  1753918954     neutral  showMe_Heaven   \n",
       "39996  1753919001        love       drapeaux   \n",
       "39997  1753919005        love       JenniRox   \n",
       "39998  1753919043   happiness       ipdaman1   \n",
       "39999  1753919049        love    Alpharalpha   \n",
       "\n",
       "                                                 content  \n",
       "0      @tiffanylue i know  i was listenin to bad habi...  \n",
       "1      Layin n bed with a headache  ughhhh...waitin o...  \n",
       "2                    Funeral ceremony...gloomy friday...  \n",
       "3                   wants to hang out with friends SOON!  \n",
       "4      @dannycastillo We want to trade with someone w...  \n",
       "...                                                  ...  \n",
       "39995                                   @JohnLloydTaylor  \n",
       "39996                     Happy Mothers Day  All my love  \n",
       "39997  Happy Mother's Day to all the mommies out ther...  \n",
       "39998  @niariley WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEE...  \n",
       "39999  @mopedronin bullet train from tokyo    the gf ...  \n",
       "\n",
       "[40000 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the dataset\n",
    "df = pd.read_csv(\"text_emotion.csv\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f8c6b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>xoshayzers</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>wannamama</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>coolfunky</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>czareaquino</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>xkilljoyx</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>1753918954</td>\n",
       "      <td>neutral</td>\n",
       "      <td>showMe_Heaven</td>\n",
       "      <td>@JohnLloydTaylor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>1753919001</td>\n",
       "      <td>love</td>\n",
       "      <td>drapeaux</td>\n",
       "      <td>Happy Mothers Day  All my love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>1753919005</td>\n",
       "      <td>love</td>\n",
       "      <td>JenniRox</td>\n",
       "      <td>Happy Mother's Day to all the mommies out ther...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>1753919043</td>\n",
       "      <td>happiness</td>\n",
       "      <td>ipdaman1</td>\n",
       "      <td>@niariley WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>1753919049</td>\n",
       "      <td>love</td>\n",
       "      <td>Alpharalpha</td>\n",
       "      <td>@mopedronin bullet train from tokyo    the gf ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id   sentiment         author  \\\n",
       "0      1956967341       empty     xoshayzers   \n",
       "1      1956967666     sadness      wannamama   \n",
       "2      1956967696     sadness      coolfunky   \n",
       "3      1956967789  enthusiasm    czareaquino   \n",
       "4      1956968416     neutral      xkilljoyx   \n",
       "...           ...         ...            ...   \n",
       "39995  1753918954     neutral  showMe_Heaven   \n",
       "39996  1753919001        love       drapeaux   \n",
       "39997  1753919005        love       JenniRox   \n",
       "39998  1753919043   happiness       ipdaman1   \n",
       "39999  1753919049        love    Alpharalpha   \n",
       "\n",
       "                                                 content  \n",
       "0      @tiffanylue i know  i was listenin to bad habi...  \n",
       "1      Layin n bed with a headache  ughhhh...waitin o...  \n",
       "2                    Funeral ceremony...gloomy friday...  \n",
       "3                   wants to hang out with friends SOON!  \n",
       "4      @dannycastillo We want to trade with someone w...  \n",
       "...                                                  ...  \n",
       "39995                                   @JohnLloydTaylor  \n",
       "39996                     Happy Mothers Day  All my love  \n",
       "39997  Happy Mother's Day to all the mommies out ther...  \n",
       "39998  @niariley WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEE...  \n",
       "39999  @mopedronin bullet train from tokyo    the gf ...  \n",
       "\n",
       "[40000 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna() # drop any missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50112b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore the tweet_id and author\n",
    "\n",
    "df.drop(['tweet_id', 'author'], axis =1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f517fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boredom</th>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>empty</th>\n",
       "      <td>827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enthusiasm</th>\n",
       "      <td>759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fun</th>\n",
       "      <td>1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>happiness</th>\n",
       "      <td>5209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hate</th>\n",
       "      <td>1323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>3842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>8638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relief</th>\n",
       "      <td>1526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>5165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>2187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worry</th>\n",
       "      <td>8459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            content\n",
       "sentiment          \n",
       "anger           110\n",
       "boredom         179\n",
       "empty           827\n",
       "enthusiasm      759\n",
       "fun            1776\n",
       "happiness      5209\n",
       "hate           1323\n",
       "love           3842\n",
       "neutral        8638\n",
       "relief         1526\n",
       "sadness        5165\n",
       "surprise       2187\n",
       "worry          8459"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check number of classes\n",
    "\n",
    "df.groupby(['sentiment']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "119e7e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>empty</td>\n",
       "      <td>tiffanylue i know  i was listenin to bad habit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhhwaitin on y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremonygloomy friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>dannycastillo We want to trade with someone wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>neutral</td>\n",
       "      <td>JohnLloydTaylor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>love</td>\n",
       "      <td>Happy Mothers Day  All my love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>love</td>\n",
       "      <td>Happy Mothers Day to all the mommies out there...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>happiness</td>\n",
       "      <td>niariley WASSUP BEAUTIFUL FOLLOW ME  PEEP OUT ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>love</td>\n",
       "      <td>mopedronin bullet train from tokyo    the gf a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentiment                                            content\n",
       "0           empty  tiffanylue i know  i was listenin to bad habit...\n",
       "1         sadness  Layin n bed with a headache  ughhhhwaitin on y...\n",
       "2         sadness                      Funeral ceremonygloomy friday\n",
       "3      enthusiasm                wants to hang out with friends SOON\n",
       "4         neutral  dannycastillo We want to trade with someone wh...\n",
       "...           ...                                                ...\n",
       "39995     neutral                                    JohnLloydTaylor\n",
       "39996        love                     Happy Mothers Day  All my love\n",
       "39997        love  Happy Mothers Day to all the mommies out there...\n",
       "39998   happiness  niariley WASSUP BEAUTIFUL FOLLOW ME  PEEP OUT ...\n",
       "39999        love  mopedronin bullet train from tokyo    the gf a...\n",
       "\n",
       "[40000 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove all punctuations from content column\n",
    "\n",
    "df['content'] = df['content'].str.replace(r'[^\\w\\s]+', '')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7df02a",
   "metadata": {},
   "source": [
    "## Calculate:\n",
    "Vocabulary size \\\n",
    "Maximum sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "019d0dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['tiffanylue i know  i was listenin to bad habit earlier and i started freakin at his part ',\n",
       "       'Layin n bed with a headache  ughhhhwaitin on your call',\n",
       "       'Funeral ceremonygloomy friday', ...,\n",
       "       'Happy Mothers Day to all the mommies out there be you woman or man as long as youre momma to someone this is your day',\n",
       "       'niariley WASSUP BEAUTIFUL FOLLOW ME  PEEP OUT MY NEW HIT SINGLES WWWMYSPACECOMIPSOHOT I DEF WAT U IN THE VIDEO',\n",
       "       'mopedronin bullet train from tokyo    the gf and i have been visiting japan since thursday  vacationsightseeing    gaijin godzilla'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# capture content in an array\n",
    "x = df.content.values\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "899e3b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since there is 13 different classes, so need to do one hot encoder\n",
    "\n",
    "y_oneHot = pd.get_dummies(df.sentiment) # Encode the classes)\n",
    "y = y_oneHot.values\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e499a172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 1,\n",
       " 'to': 2,\n",
       " 'the': 3,\n",
       " 'a': 4,\n",
       " 'my': 5,\n",
       " 'and': 6,\n",
       " 'you': 7,\n",
       " 'it': 8,\n",
       " 'is': 9,\n",
       " 'in': 10,\n",
       " 'for': 11,\n",
       " 'of': 12,\n",
       " 'im': 13,\n",
       " 'on': 14,\n",
       " 'me': 15,\n",
       " 'have': 16,\n",
       " 'so': 17,\n",
       " 'that': 18,\n",
       " 'but': 19,\n",
       " 'just': 20,\n",
       " 'day': 21,\n",
       " 'with': 22,\n",
       " 'be': 23,\n",
       " 'its': 24,\n",
       " 'at': 25,\n",
       " 'was': 26,\n",
       " 'not': 27,\n",
       " 'all': 28,\n",
       " 'good': 29,\n",
       " 'this': 30,\n",
       " 'now': 31,\n",
       " 'out': 32,\n",
       " 'up': 33,\n",
       " 'get': 34,\n",
       " 'like': 35,\n",
       " 'are': 36,\n",
       " 'no': 37,\n",
       " 'go': 38,\n",
       " 'dont': 39,\n",
       " 'do': 40,\n",
       " 'your': 41,\n",
       " 'love': 42,\n",
       " 'got': 43,\n",
       " 'too': 44,\n",
       " 'work': 45,\n",
       " 'today': 46,\n",
       " 'going': 47,\n",
       " 'cant': 48,\n",
       " 'happy': 49,\n",
       " 'from': 50,\n",
       " 'one': 51,\n",
       " 'time': 52,\n",
       " 'lol': 53,\n",
       " 'know': 54,\n",
       " 'back': 55,\n",
       " 'what': 56,\n",
       " 'u': 57,\n",
       " 'will': 58,\n",
       " 'really': 59,\n",
       " 'about': 60,\n",
       " 'we': 61,\n",
       " 'am': 62,\n",
       " 'see': 63,\n",
       " 'had': 64,\n",
       " 'there': 65,\n",
       " 'can': 66,\n",
       " 'some': 67,\n",
       " 'mothers': 68,\n",
       " 'if': 69,\n",
       " 'well': 70,\n",
       " 'new': 71,\n",
       " 'night': 72,\n",
       " 'home': 73,\n",
       " 'think': 74,\n",
       " 'as': 75,\n",
       " 'want': 76,\n",
       " 'when': 77,\n",
       " 'thanks': 78,\n",
       " 'how': 79,\n",
       " 'amp': 80,\n",
       " 'oh': 81,\n",
       " 'still': 82,\n",
       " 'off': 83,\n",
       " 'much': 84,\n",
       " 'an': 85,\n",
       " '2': 86,\n",
       " 'they': 87,\n",
       " 'more': 88,\n",
       " 'miss': 89,\n",
       " 'here': 90,\n",
       " 'great': 91,\n",
       " 'last': 92,\n",
       " 'need': 93,\n",
       " 'morning': 94,\n",
       " 'hope': 95,\n",
       " 'has': 96,\n",
       " 'her': 97,\n",
       " 'thats': 98,\n",
       " 'been': 99,\n",
       " 'haha': 100,\n",
       " 'ill': 101,\n",
       " 'feel': 102,\n",
       " 'then': 103,\n",
       " 'fun': 104,\n",
       " 'twitter': 105,\n",
       " 'or': 106,\n",
       " 'only': 107,\n",
       " 'again': 108,\n",
       " 'would': 109,\n",
       " 'why': 110,\n",
       " 'sad': 111,\n",
       " 'he': 112,\n",
       " 'wish': 113,\n",
       " 'tomorrow': 114,\n",
       " 'didnt': 115,\n",
       " 'sorry': 116,\n",
       " 'tonight': 117,\n",
       " 'bad': 118,\n",
       " 'right': 119,\n",
       " 'very': 120,\n",
       " 'were': 121,\n",
       " 'by': 122,\n",
       " 'did': 123,\n",
       " 'make': 124,\n",
       " 'them': 125,\n",
       " 'nice': 126,\n",
       " 'better': 127,\n",
       " 'though': 128,\n",
       " 'she': 129,\n",
       " 'gonna': 130,\n",
       " 'sleep': 131,\n",
       " 'ive': 132,\n",
       " 'yeah': 133,\n",
       " 'should': 134,\n",
       " 'getting': 135,\n",
       " 'way': 136,\n",
       " 'week': 137,\n",
       " 'could': 138,\n",
       " 'over': 139,\n",
       " 'weekend': 140,\n",
       " 'come': 141,\n",
       " 'people': 142,\n",
       " 'next': 143,\n",
       " 'youre': 144,\n",
       " 'bed': 145,\n",
       " 'watching': 146,\n",
       " 'after': 147,\n",
       " 'school': 148,\n",
       " 'awesome': 149,\n",
       " 'hate': 150,\n",
       " 'him': 151,\n",
       " 'wait': 152,\n",
       " 'days': 153,\n",
       " 'hey': 154,\n",
       " 'even': 155,\n",
       " 'say': 156,\n",
       " 'little': 157,\n",
       " 'best': 158,\n",
       " 'never': 159,\n",
       " 'long': 160,\n",
       " 'being': 161,\n",
       " 'thank': 162,\n",
       " 'wont': 163,\n",
       " 'soon': 164,\n",
       " 'working': 165,\n",
       " 'yes': 166,\n",
       " 'down': 167,\n",
       " 'thing': 168,\n",
       " 'having': 169,\n",
       " 'show': 170,\n",
       " 'his': 171,\n",
       " 'mom': 172,\n",
       " 'who': 173,\n",
       " 'take': 174,\n",
       " 'ok': 175,\n",
       " 'please': 176,\n",
       " 'any': 177,\n",
       " 'done': 178,\n",
       " 'sure': 179,\n",
       " 'always': 180,\n",
       " 'than': 181,\n",
       " '4': 182,\n",
       " 'our': 183,\n",
       " 'look': 184,\n",
       " 'doing': 185,\n",
       " 'first': 186,\n",
       " 'life': 187,\n",
       " 'friends': 188,\n",
       " 'sick': 189,\n",
       " 'wanna': 190,\n",
       " 'ur': 191,\n",
       " 'may': 192,\n",
       " 'tired': 193,\n",
       " 'everyone': 194,\n",
       " 'because': 195,\n",
       " 'cool': 196,\n",
       " 'feeling': 197,\n",
       " 'guys': 198,\n",
       " 'already': 199,\n",
       " 'find': 200,\n",
       " 'movie': 201,\n",
       " 'phone': 202,\n",
       " 'doesnt': 203,\n",
       " 'made': 204,\n",
       " 'another': 205,\n",
       " 'something': 206,\n",
       " 'watch': 207,\n",
       " '3': 208,\n",
       " 'where': 209,\n",
       " 'us': 210,\n",
       " 'hours': 211,\n",
       " 'trying': 212,\n",
       " 'man': 213,\n",
       " 'yet': 214,\n",
       " 'before': 215,\n",
       " 'ever': 216,\n",
       " 'yay': 217,\n",
       " 'ready': 218,\n",
       " 'house': 219,\n",
       " 'x': 220,\n",
       " 'looking': 221,\n",
       " 'pretty': 222,\n",
       " 'old': 223,\n",
       " 'sucks': 224,\n",
       " 'thought': 225,\n",
       " 'live': 226,\n",
       " 'maybe': 227,\n",
       " 'went': 228,\n",
       " 'friday': 229,\n",
       " 'girl': 230,\n",
       " 'finally': 231,\n",
       " 'left': 232,\n",
       " 'let': 233,\n",
       " 'guess': 234,\n",
       " 'into': 235,\n",
       " 'away': 236,\n",
       " 'same': 237,\n",
       " 'damn': 238,\n",
       " 'follow': 239,\n",
       " 'big': 240,\n",
       " 'bit': 241,\n",
       " 'other': 242,\n",
       " 'friend': 243,\n",
       " 'someone': 244,\n",
       " 'amazing': 245,\n",
       " 'nothing': 246,\n",
       " 'missed': 247,\n",
       " 'looks': 248,\n",
       " 'lt3': 249,\n",
       " 'things': 250,\n",
       " 'monday': 251,\n",
       " 'keep': 252,\n",
       " 'also': 253,\n",
       " 'omg': 254,\n",
       " 'year': 255,\n",
       " 'while': 256,\n",
       " '1': 257,\n",
       " 'said': 258,\n",
       " 'wow': 259,\n",
       " 'two': 260,\n",
       " 'hear': 261,\n",
       " 'those': 262,\n",
       " 'hot': 263,\n",
       " 'star': 264,\n",
       " 'later': 265,\n",
       " 'actually': 266,\n",
       " 'try': 267,\n",
       " 'hard': 268,\n",
       " 'glad': 269,\n",
       " 'tell': 270,\n",
       " 'saw': 271,\n",
       " 'bored': 272,\n",
       " 'havent': 273,\n",
       " 'lost': 274,\n",
       " 'ya': 275,\n",
       " 'tweet': 276,\n",
       " 'call': 277,\n",
       " 'early': 278,\n",
       " 'such': 279,\n",
       " 'car': 280,\n",
       " 'baby': 281,\n",
       " 'ugh': 282,\n",
       " 'start': 283,\n",
       " 'help': 284,\n",
       " 'job': 285,\n",
       " 'makes': 286,\n",
       " 'hi': 287,\n",
       " 'isnt': 288,\n",
       " 'hes': 289,\n",
       " 'sun': 290,\n",
       " 'n': 291,\n",
       " 'song': 292,\n",
       " 'coming': 293,\n",
       " 'birthday': 294,\n",
       " '5': 295,\n",
       " 'might': 296,\n",
       " 'until': 297,\n",
       " 'head': 298,\n",
       " 'world': 299,\n",
       " 'weather': 300,\n",
       " 'id': 301,\n",
       " 'waiting': 302,\n",
       " 'around': 303,\n",
       " 'stuff': 304,\n",
       " 'gone': 305,\n",
       " 'rain': 306,\n",
       " 'few': 307,\n",
       " 'play': 308,\n",
       " 'since': 309,\n",
       " 'must': 310,\n",
       " 'myself': 311,\n",
       " 'does': 312,\n",
       " 'god': 313,\n",
       " 'lot': 314,\n",
       " 'gotta': 315,\n",
       " 'excited': 316,\n",
       " 'poor': 317,\n",
       " 'anything': 318,\n",
       " 'making': 319,\n",
       " 'read': 320,\n",
       " 'party': 321,\n",
       " 'yesterday': 322,\n",
       " 'moms': 323,\n",
       " 'check': 324,\n",
       " 'late': 325,\n",
       " 'hair': 326,\n",
       " 'till': 327,\n",
       " 'aww': 328,\n",
       " 'their': 329,\n",
       " 'family': 330,\n",
       " 'put': 331,\n",
       " 'found': 332,\n",
       " 'give': 333,\n",
       " 'music': 334,\n",
       " 'least': 335,\n",
       " 'summer': 336,\n",
       " 'mean': 337,\n",
       " 'enjoy': 338,\n",
       " 'almost': 339,\n",
       " 'talk': 340,\n",
       " 'anyone': 341,\n",
       " 'without': 342,\n",
       " 'welcome': 343,\n",
       " 'sunday': 344,\n",
       " 'leave': 345,\n",
       " 'listening': 346,\n",
       " 'cause': 347,\n",
       " 'end': 348,\n",
       " 'hurts': 349,\n",
       " 'funny': 350,\n",
       " 'cute': 351,\n",
       " 'totally': 352,\n",
       " 'everything': 353,\n",
       " 'most': 354,\n",
       " 'lunch': 355,\n",
       " 'wanted': 356,\n",
       " 'dinner': 357,\n",
       " 'eat': 358,\n",
       " 'money': 359,\n",
       " 'whats': 360,\n",
       " 'tho': 361,\n",
       " 'game': 362,\n",
       " 'thinking': 363,\n",
       " 'shes': 364,\n",
       " 'many': 365,\n",
       " 'wasnt': 366,\n",
       " 'forward': 367,\n",
       " 'free': 368,\n",
       " 'finished': 369,\n",
       " 'far': 370,\n",
       " 'sounds': 371,\n",
       " 'use': 372,\n",
       " 'believe': 373,\n",
       " 'luck': 374,\n",
       " 'missing': 375,\n",
       " 'hour': 376,\n",
       " 'stupid': 377,\n",
       " 'probably': 378,\n",
       " 'beautiful': 379,\n",
       " 'playing': 380,\n",
       " 'through': 381,\n",
       " 'stop': 382,\n",
       " 'mine': 383,\n",
       " 'xx': 384,\n",
       " 'sweet': 385,\n",
       " 'times': 386,\n",
       " 'place': 387,\n",
       " 'food': 388,\n",
       " 'these': 389,\n",
       " 'cold': 390,\n",
       " 'okay': 391,\n",
       " 'theres': 392,\n",
       " 'coffee': 393,\n",
       " 'lovely': 394,\n",
       " 'shit': 395,\n",
       " 'every': 396,\n",
       " 'couldnt': 397,\n",
       " 'which': 398,\n",
       " 'ha': 399,\n",
       " 'enough': 400,\n",
       " 'eating': 401,\n",
       " 'kids': 402,\n",
       " 'followers': 403,\n",
       " 'stuck': 404,\n",
       " 'outside': 405,\n",
       " 'says': 406,\n",
       " 'took': 407,\n",
       " 'hahaha': 408,\n",
       " 'seen': 409,\n",
       " 'kinda': 410,\n",
       " 'r': 411,\n",
       " 'w': 412,\n",
       " 'whole': 413,\n",
       " 'real': 414,\n",
       " 'office': 415,\n",
       " 'awww': 416,\n",
       " 'headache': 417,\n",
       " 'wants': 418,\n",
       " 'sooo': 419,\n",
       " 'came': 420,\n",
       " 'woke': 421,\n",
       " 'years': 422,\n",
       " 'pic': 423,\n",
       " 'room': 424,\n",
       " 'weeks': 425,\n",
       " 'buy': 426,\n",
       " 'hell': 427,\n",
       " 'anymore': 428,\n",
       " 'following': 429,\n",
       " 'wrong': 430,\n",
       " 'forgot': 431,\n",
       " 'stay': 432,\n",
       " 'o': 433,\n",
       " '10': 434,\n",
       " 'meet': 435,\n",
       " 'video': 436,\n",
       " 'both': 437,\n",
       " 'b': 438,\n",
       " 'name': 439,\n",
       " 'taking': 440,\n",
       " 'd': 441,\n",
       " 'ago': 442,\n",
       " 'tv': 443,\n",
       " 'loved': 444,\n",
       " 'theyre': 445,\n",
       " 'guy': 446,\n",
       " 'sitting': 447,\n",
       " 'able': 448,\n",
       " 'full': 449,\n",
       " 'else': 450,\n",
       " 'class': 451,\n",
       " 'goodnight': 452,\n",
       " 'busy': 453,\n",
       " 'book': 454,\n",
       " 'super': 455,\n",
       " 'hopefully': 456,\n",
       " 'tweets': 457,\n",
       " 'alone': 458,\n",
       " 'online': 459,\n",
       " 'hit': 460,\n",
       " 'seems': 461,\n",
       " 'shopping': 462,\n",
       " 'lots': 463,\n",
       " 'used': 464,\n",
       " 'internet': 465,\n",
       " 'iphone': 466,\n",
       " 'post': 467,\n",
       " 'half': 468,\n",
       " 'own': 469,\n",
       " 'gets': 470,\n",
       " 'cry': 471,\n",
       " 'told': 472,\n",
       " 'holiday': 473,\n",
       " 'dad': 474,\n",
       " 'quite': 475,\n",
       " 'send': 476,\n",
       " 'idea': 477,\n",
       " 'wars': 478,\n",
       " 'talking': 479,\n",
       " 'called': 480,\n",
       " 'boo': 481,\n",
       " 'news': 482,\n",
       " 'trek': 483,\n",
       " 'hurt': 484,\n",
       " 'hello': 485,\n",
       " 'dude': 486,\n",
       " 'dog': 487,\n",
       " 'remember': 488,\n",
       " 'saturday': 489,\n",
       " 'mum': 490,\n",
       " 'change': 491,\n",
       " 'break': 492,\n",
       " 'seeing': 493,\n",
       " 'fucking': 494,\n",
       " 'computer': 495,\n",
       " '6': 496,\n",
       " 'minutes': 497,\n",
       " 'bank': 498,\n",
       " 'once': 499,\n",
       " 'cuz': 500,\n",
       " 'boy': 501,\n",
       " 'btw': 502,\n",
       " 'ah': 503,\n",
       " 'feels': 504,\n",
       " 'win': 505,\n",
       " 'tried': 506,\n",
       " 'watched': 507,\n",
       " 'rest': 508,\n",
       " 'run': 509,\n",
       " 'either': 510,\n",
       " 'hehe': 511,\n",
       " 'crazy': 512,\n",
       " 'heard': 513,\n",
       " 'awake': 514,\n",
       " 'kind': 515,\n",
       " 'starting': 516,\n",
       " 'raining': 517,\n",
       " 'wonderful': 518,\n",
       " 'lucky': 519,\n",
       " 'breakfast': 520,\n",
       " 'picture': 521,\n",
       " 'part': 522,\n",
       " 'girls': 523,\n",
       " 'face': 524,\n",
       " 'broke': 525,\n",
       " 'soo': 526,\n",
       " 'site': 527,\n",
       " 'til': 528,\n",
       " 'la': 529,\n",
       " 'evening': 530,\n",
       " 'mind': 531,\n",
       " 'drink': 532,\n",
       " 'instead': 533,\n",
       " 'high': 534,\n",
       " 'trip': 535,\n",
       " 'beach': 536,\n",
       " 'leaving': 537,\n",
       " 'true': 538,\n",
       " 'blog': 539,\n",
       " 'ones': 540,\n",
       " 'person': 541,\n",
       " 'drive': 542,\n",
       " 'pain': 543,\n",
       " 'aw': 544,\n",
       " 'close': 545,\n",
       " 'email': 546,\n",
       " 'concert': 547,\n",
       " 'months': 548,\n",
       " 'heart': 549,\n",
       " 'facebook': 550,\n",
       " 'link': 551,\n",
       " 'fail': 552,\n",
       " 'dead': 553,\n",
       " 'set': 554,\n",
       " 'turn': 555,\n",
       " 'lets': 556,\n",
       " 'bought': 557,\n",
       " 'lil': 558,\n",
       " 'yall': 559,\n",
       " 'loves': 560,\n",
       " 'mother': 561,\n",
       " 'lmao': 562,\n",
       " 'move': 563,\n",
       " 'suck': 564,\n",
       " 'fuck': 565,\n",
       " 'wishing': 566,\n",
       " 'sometimes': 567,\n",
       " 'goes': 568,\n",
       " '8': 569,\n",
       " 'fine': 570,\n",
       " '7': 571,\n",
       " 'ask': 572,\n",
       " 'card': 573,\n",
       " 'perfect': 574,\n",
       " 'youtube': 575,\n",
       " 'p': 576,\n",
       " 'soooo': 577,\n",
       " 'hungry': 578,\n",
       " 'writing': 579,\n",
       " 'started': 580,\n",
       " 'hoping': 581,\n",
       " 'yea': 582,\n",
       " 'happened': 583,\n",
       " 'running': 584,\n",
       " 'bout': 585,\n",
       " 'favorite': 586,\n",
       " 'month': 587,\n",
       " 'comes': 588,\n",
       " 'ice': 589,\n",
       " 'heading': 590,\n",
       " 'xd': 591,\n",
       " 'fan': 592,\n",
       " 'tea': 593,\n",
       " 'definitely': 594,\n",
       " 'needs': 595,\n",
       " 'care': 596,\n",
       " 'exam': 597,\n",
       " 'shower': 598,\n",
       " 'arent': 599,\n",
       " 'couple': 600,\n",
       " 'anyway': 601,\n",
       " 'asleep': 602,\n",
       " 's': 603,\n",
       " 'problem': 604,\n",
       " 'pics': 605,\n",
       " 'test': 606,\n",
       " 'mommy': 607,\n",
       " 'bday': 608,\n",
       " 'boring': 609,\n",
       " 'city': 610,\n",
       " 'wake': 611,\n",
       " 'sunny': 612,\n",
       " 'tickets': 613,\n",
       " 'sore': 614,\n",
       " 'cat': 615,\n",
       " 'course': 616,\n",
       " 'movies': 617,\n",
       " 'means': 618,\n",
       " '4th': 619,\n",
       " 'moment': 620,\n",
       " 'gt': 621,\n",
       " 'write': 622,\n",
       " 'moving': 623,\n",
       " 'reply': 624,\n",
       " 'reading': 625,\n",
       " 'ass': 626,\n",
       " 'seriously': 627,\n",
       " 'text': 628,\n",
       " 'june': 629,\n",
       " 'open': 630,\n",
       " 'less': 631,\n",
       " 'nite': 632,\n",
       " 'supposed': 633,\n",
       " 'crap': 634,\n",
       " 'ipod': 635,\n",
       " 'youll': 636,\n",
       " 'catch': 637,\n",
       " 'chocolate': 638,\n",
       " 'jealous': 639,\n",
       " 'gave': 640,\n",
       " 'died': 641,\n",
       " 'songs': 642,\n",
       " 'laptop': 643,\n",
       " 'goin': 644,\n",
       " 'meeting': 645,\n",
       " 'tommcfly': 646,\n",
       " 'works': 647,\n",
       " 'sleeping': 648,\n",
       " 'top': 649,\n",
       " 'spend': 650,\n",
       " 'rock': 651,\n",
       " 'yep': 652,\n",
       " 'sigh': 653,\n",
       " 'vote': 654,\n",
       " 'happen': 655,\n",
       " 'uk': 656,\n",
       " 'mad': 657,\n",
       " 'wouldnt': 658,\n",
       " 'reason': 659,\n",
       " 'red': 660,\n",
       " 'dream': 661,\n",
       " 'season': 662,\n",
       " 'special': 663,\n",
       " 'list': 664,\n",
       " 'eyes': 665,\n",
       " 'dear': 666,\n",
       " 'fast': 667,\n",
       " 'cleaning': 668,\n",
       " 'cut': 669,\n",
       " 'fb': 670,\n",
       " 'together': 671,\n",
       " 'saying': 672,\n",
       " 'using': 673,\n",
       " 'bye': 674,\n",
       " 'hang': 675,\n",
       " 'due': 676,\n",
       " 'account': 677,\n",
       " 'em': 678,\n",
       " 'interesting': 679,\n",
       " 'forget': 680,\n",
       " 'xxx': 681,\n",
       " 'enjoying': 682,\n",
       " 'mitchelmusso': 683,\n",
       " 'photo': 684,\n",
       " 'short': 685,\n",
       " 'bring': 686,\n",
       " 'sadly': 687,\n",
       " 'driving': 688,\n",
       " 'water': 689,\n",
       " 'knew': 690,\n",
       " 'second': 691,\n",
       " 'finish': 692,\n",
       " 'sound': 693,\n",
       " 'hugs': 694,\n",
       " 'black': 695,\n",
       " 'boys': 696,\n",
       " 'broken': 697,\n",
       " 'pictures': 698,\n",
       " 'exams': 699,\n",
       " 'nap': 700,\n",
       " 'past': 701,\n",
       " 'final': 702,\n",
       " 'town': 703,\n",
       " 'plans': 704,\n",
       " 'fell': 705,\n",
       " 'gym': 706,\n",
       " 'jonas': 707,\n",
       " '1st': 708,\n",
       " 'aint': 709,\n",
       " 'tour': 710,\n",
       " 'warm': 711,\n",
       " 'weird': 712,\n",
       " '12': 713,\n",
       " 'agree': 714,\n",
       " 'ride': 715,\n",
       " 'worth': 716,\n",
       " 'c': 717,\n",
       " 'clean': 718,\n",
       " 'listen': 719,\n",
       " 'da': 720,\n",
       " 'spent': 721,\n",
       " 'cream': 722,\n",
       " 'ppl': 723,\n",
       " 'episode': 724,\n",
       " 'date': 725,\n",
       " 'seem': 726,\n",
       " 'afternoon': 727,\n",
       " 'liked': 728,\n",
       " '30': 729,\n",
       " 'sent': 730,\n",
       " 'youve': 731,\n",
       " 'sister': 732,\n",
       " 'photos': 733,\n",
       " 'mood': 734,\n",
       " 'upset': 735,\n",
       " 'different': 736,\n",
       " 'park': 737,\n",
       " 'london': 738,\n",
       " 'lady': 739,\n",
       " 'ate': 740,\n",
       " 'flu': 741,\n",
       " 'under': 742,\n",
       " 'pizza': 743,\n",
       " 'google': 744,\n",
       " 'drinking': 745,\n",
       " 'unfortunately': 746,\n",
       " 'walk': 747,\n",
       " 'nope': 748,\n",
       " 'fair': 749,\n",
       " 'bgt': 750,\n",
       " 'ahh': 751,\n",
       " 'horrible': 752,\n",
       " 'rather': 753,\n",
       " 'fall': 754,\n",
       " 'dance': 755,\n",
       " 'slow': 756,\n",
       " 'cake': 757,\n",
       " 'study': 758,\n",
       " 'page': 759,\n",
       " 'club': 760,\n",
       " 'worse': 761,\n",
       " 'shows': 762,\n",
       " 'mileycyrus': 763,\n",
       " 'huge': 764,\n",
       " 'beer': 765,\n",
       " 'die': 766,\n",
       " 'mr': 767,\n",
       " 'sunshine': 768,\n",
       " 'plan': 769,\n",
       " 'inside': 770,\n",
       " 'hug': 771,\n",
       " 'garden': 772,\n",
       " 'wtf': 773,\n",
       " 'y': 774,\n",
       " 'three': 775,\n",
       " 'david': 776,\n",
       " 'store': 777,\n",
       " 'wear': 778,\n",
       " 'add': 779,\n",
       " 'hmm': 780,\n",
       " 'wedding': 781,\n",
       " 'congrats': 782,\n",
       " 'played': 783,\n",
       " 'fans': 784,\n",
       " 'except': 785,\n",
       " 'apparently': 786,\n",
       " 'chance': 787,\n",
       " 'meant': 788,\n",
       " 'babe': 789,\n",
       " 'brother': 790,\n",
       " 'worry': 791,\n",
       " 'message': 792,\n",
       " 'bus': 793,\n",
       " 'visit': 794,\n",
       " 'update': 795,\n",
       " 'plus': 796,\n",
       " 'looked': 797,\n",
       " 'loving': 798,\n",
       " 'shame': 799,\n",
       " 'kid': 800,\n",
       " 'point': 801,\n",
       " 'learn': 802,\n",
       " 'album': 803,\n",
       " '100': 804,\n",
       " 'side': 805,\n",
       " 'english': 806,\n",
       " 'white': 807,\n",
       " 'earlier': 808,\n",
       " 'via': 809,\n",
       " 'slept': 810,\n",
       " 'worst': 811,\n",
       " 'throat': 812,\n",
       " 'yours': 813,\n",
       " 'completely': 814,\n",
       " 'save': 815,\n",
       " 'answer': 816,\n",
       " 'words': 817,\n",
       " 'figure': 818,\n",
       " 'everybody': 819,\n",
       " 'tweeting': 820,\n",
       " 'sat': 821,\n",
       " 'traffic': 822,\n",
       " 'bc': 823,\n",
       " 'story': 824,\n",
       " 'especially': 825,\n",
       " 'living': 826,\n",
       " 'absolutely': 827,\n",
       " 'updates': 828,\n",
       " 'thx': 829,\n",
       " 'needed': 830,\n",
       " 'idk': 831,\n",
       " 'drunk': 832,\n",
       " 'happens': 833,\n",
       " 'parents': 834,\n",
       " 'kill': 835,\n",
       " 'ahhh': 836,\n",
       " 'near': 837,\n",
       " 'guitar': 838,\n",
       " 'hand': 839,\n",
       " 'son': 840,\n",
       " 'brothers': 841,\n",
       " 'ohh': 842,\n",
       " 'hows': 843,\n",
       " 'mac': 844,\n",
       " 'longer': 845,\n",
       " 'during': 846,\n",
       " 'smile': 847,\n",
       " 'snl': 848,\n",
       " 'prom': 849,\n",
       " 'starts': 850,\n",
       " 'bet': 851,\n",
       " '20': 852,\n",
       " 'sign': 853,\n",
       " 'word': 854,\n",
       " 'business': 855,\n",
       " 'along': 856,\n",
       " 'green': 857,\n",
       " 'won': 858,\n",
       " 'yummy': 859,\n",
       " 'relaxing': 860,\n",
       " 'line': 861,\n",
       " 'project': 862,\n",
       " 'each': 863,\n",
       " 'blue': 864,\n",
       " 'crying': 865,\n",
       " 'feet': 866,\n",
       " 'gift': 867,\n",
       " 'easy': 868,\n",
       " 'dreams': 869,\n",
       " 'radio': 870,\n",
       " 'tom': 871,\n",
       " 'lame': 872,\n",
       " 'mums': 873,\n",
       " 'dang': 874,\n",
       " 'felt': 875,\n",
       " 'ddlovato': 876,\n",
       " 'posted': 877,\n",
       " 'wishes': 878,\n",
       " 'vegas': 879,\n",
       " '2nd': 880,\n",
       " 'chat': 881,\n",
       " 'bike': 882,\n",
       " 'camera': 883,\n",
       " 'fingers': 884,\n",
       " 'thinks': 885,\n",
       " 'met': 886,\n",
       " 'homework': 887,\n",
       " 'hanging': 888,\n",
       " 'hates': 889,\n",
       " 'number': 890,\n",
       " 'lazy': 891,\n",
       " '15': 892,\n",
       " 'film': 893,\n",
       " 'behind': 894,\n",
       " 'comment': 895,\n",
       " 'chicken': 896,\n",
       " 'fact': 897,\n",
       " 'air': 898,\n",
       " 'yourself': 899,\n",
       " 'whatever': 900,\n",
       " 'wondering': 901,\n",
       " 'realized': 902,\n",
       " 'cannot': 903,\n",
       " 'itll': 904,\n",
       " 'wonder': 905,\n",
       " 'lonely': 906,\n",
       " 'worked': 907,\n",
       " 'shoes': 908,\n",
       " 'hun': 909,\n",
       " 'cheese': 910,\n",
       " 'company': 911,\n",
       " 'join': 912,\n",
       " 'm': 913,\n",
       " 'understand': 914,\n",
       " 'band': 915,\n",
       " 'although': 916,\n",
       " 'fantastic': 917,\n",
       " 'bless': 918,\n",
       " 'apple': 919,\n",
       " 'stomach': 920,\n",
       " 'version': 921,\n",
       " 'staying': 922,\n",
       " 'sexy': 923,\n",
       " 'forever': 924,\n",
       " 'officially': 925,\n",
       " 'small': 926,\n",
       " 'luv': 927,\n",
       " 'mama': 928,\n",
       " 'yum': 929,\n",
       " 'paper': 930,\n",
       " 'windows': 931,\n",
       " 'app': 932,\n",
       " 'body': 933,\n",
       " 'cos': 934,\n",
       " 'sold': 935,\n",
       " 'ps': 936,\n",
       " 'woo': 937,\n",
       " 'sleepy': 938,\n",
       " 'middle': 939,\n",
       " 'wine': 940,\n",
       " '2day': 941,\n",
       " 'wife': 942,\n",
       " 'taken': 943,\n",
       " 'fly': 944,\n",
       " 'tummy': 945,\n",
       " 'safe': 946,\n",
       " 'proud': 947,\n",
       " 'pool': 948,\n",
       " 'pick': 949,\n",
       " 'single': 950,\n",
       " 'usually': 951,\n",
       " 'college': 952,\n",
       " 'pass': 953,\n",
       " 'thursday': 954,\n",
       " 'appreciate': 955,\n",
       " 'ran': 956,\n",
       " 'chillin': 957,\n",
       " 'church': 958,\n",
       " 'quick': 959,\n",
       " 'none': 960,\n",
       " 'keeps': 961,\n",
       " 'john': 962,\n",
       " 'nick': 963,\n",
       " 'k': 964,\n",
       " 'shop': 965,\n",
       " 'exactly': 966,\n",
       " 'scared': 967,\n",
       " 'bbq': 968,\n",
       " 'rainy': 969,\n",
       " 'minute': 970,\n",
       " 'boyfriend': 971,\n",
       " 'daughter': 972,\n",
       " 'walking': 973,\n",
       " 'closed': 974,\n",
       " 'ima': 975,\n",
       " 'power': 976,\n",
       " 'chinese': 977,\n",
       " 'support': 978,\n",
       " 'hahah': 979,\n",
       " 'hubby': 980,\n",
       " 'team': 981,\n",
       " 'pay': 982,\n",
       " 'graduation': 983,\n",
       " 'clothes': 984,\n",
       " 'deal': 985,\n",
       " 'rip': 986,\n",
       " '9': 987,\n",
       " 'messages': 988,\n",
       " 'dm': 989,\n",
       " 'sending': 990,\n",
       " 'flight': 991,\n",
       " 'spending': 992,\n",
       " 'nights': 993,\n",
       " 'cup': 994,\n",
       " 'shirt': 995,\n",
       " 'jonasbrothers': 996,\n",
       " 'front': 997,\n",
       " 'tuesday': 998,\n",
       " 'problems': 999,\n",
       " 'hmmm': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a token for each word \n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x)\n",
    "\n",
    "tokens = tokenizer.word_index\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5c8d805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53364"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the vocabulary size\n",
    "\n",
    "vocabulary_size = len(tokens)+1\n",
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "822cec4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[15139, 1, 54, 1, 26, 3169, 2, 118, 5160, 808, 6, 1, 580, 1110, 25, 171, 522],\n",
       " [9585, 291, 145, 22, 4, 417, 15140, 14, 41, 277],\n",
       " [2805, 15141, 229],\n",
       " [418, 2, 675, 32, 22, 188, 164],\n",
       " [15142, 61, 76, 2, 3170, 22, 244, 173, 96, 2506, 613, 19, 37, 51, 58],\n",
       " [7300, 15143, 110, 115, 7, 38, 2, 849, 823, 5, 1438, 115, 35, 5, 188],\n",
       " [1,\n",
       "  134,\n",
       "  23,\n",
       "  131,\n",
       "  19,\n",
       "  13,\n",
       "  27,\n",
       "  363,\n",
       "  60,\n",
       "  85,\n",
       "  223,\n",
       "  243,\n",
       "  173,\n",
       "  1,\n",
       "  76,\n",
       "  19,\n",
       "  289,\n",
       "  1568,\n",
       "  31,\n",
       "  238,\n",
       "  80,\n",
       "  112,\n",
       "  418,\n",
       "  15,\n",
       "  86,\n",
       "  7301],\n",
       " [1000, 15144, 9, 167],\n",
       " [15145, 15146, 5, 42, 1, 89, 7],\n",
       " [15147, 13, 116, 25, 335, 24, 229]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the tweets into sequences of tokens\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(x)\n",
    "sequences[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bc5b01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum sequence length  33\n"
     ]
    }
   ],
   "source": [
    "# Use a loop to find the longest subsequence\n",
    "\n",
    "sequence_length = max( len(sequence) for sequence in sequences )\n",
    "print('maximum sequence length ', sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1d9b1c",
   "metadata": {},
   "source": [
    "## Divide your dataset into training and testing sets:\n",
    "70% training set \\\n",
    "30% Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1048820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into testing 30% and training 70% splits\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(sequences, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7f9e759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add padding and move the content to the end of the strings\n",
    "\n",
    "from keras.utils import pad_sequences \n",
    "\n",
    "x_train  = pad_sequences(x_train, maxlen = sequence_length, padding = \"pre\")\n",
    "x_test  = pad_sequences(x_test, maxlen = sequence_length, padding = \"pre\")\n",
    "# using pre puts all predictions at the last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd356006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,   307,   425,   591],\n",
       "       [    0,     0,     0, ...,   397,  3292,     8],\n",
       "       [    0,     0,     0, ...,     1,    54,    53],\n",
       "       ...,\n",
       "       [    0,     0,     0, ..., 43978,   423,   176],\n",
       "       [    0,     0,     0, ...,    33,    25,  2839],\n",
       "       [    0,     0,     0, ...,  1978,  3446,   552]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b081a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28000, 33), (28000, 13), (12000, 33), (12000, 13))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e38d8a",
   "metadata": {},
   "source": [
    "# 3. Define your Recurrent Neural Network\n",
    "Define an RNN with the following layers:\n",
    "\n",
    "The input layer is an embedding layer with the following parameters: \\\n",
    "the input dimension is the vocabulary size; \\\n",
    "the output dimension is 10; \\\n",
    "the input length is the maximum sequence length;\n",
    "\n",
    "Define an LSTM layer with 128 units; \\\n",
    "Define an LSTM layer with 64 units;\n",
    "\n",
    "Define a fully connected layer with: \\\n",
    "100 units; \\\n",
    "activation function: ReLU;\n",
    " \n",
    "A Dropout layer with 0.5 rate;\n",
    " \n",
    "The output layer is a fully-connected layer with: \\\n",
    "5 units and \\\n",
    "activation function: softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff9fb1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "\n",
    "from keras.models import Sequential\n",
    "model = Sequential()\n",
    "\n",
    "# add the layers\n",
    "\n",
    "from keras.layers import Embedding, LSTM, Dropout, Dense\n",
    "\n",
    "model.add(Embedding(input_dim = vocabulary_size,\n",
    "                   output_dim = 10,\n",
    "                   input_length = sequence_length))\n",
    "\n",
    "model.add(LSTM(units = 128))\n",
    "\n",
    "#model.add(LSTM(units = 64)) \n",
    "# this layer creates error 'Input 0 of layer \"lstm_2\" is incompatible with the layer: \n",
    "# expected ndim=3, found ndim=2. Full shape received: (None, 128)\n",
    "\n",
    "model.add(Dense(units = 100, activation = \"relu\"))\n",
    "\n",
    "model.add(Dropout(rate = 0.5))\n",
    "\n",
    "model.add(Dense(units = 13, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb067063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 33, 10)            533640    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               71168     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               12900     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 13)                1313      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 619,021\n",
      "Trainable params: 619,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8371f99e",
   "metadata": {},
   "source": [
    "# 4. Choosing Hyperparameters\n",
    "Build the network using the following parameters:\\\n",
    " Optimizer: Adam \\\n",
    "Loss function: categorical_crossentropy \\\n",
    "Metrics: accuracy \\\n",
    "Batch size: 256 \\\n",
    "Epochs: 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d4bfe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = \"categorical_crossentropy\", metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9982fbc5",
   "metadata": {},
   "source": [
    "# 5. Training Network\n",
    "Use Keras to implement the network described and train your data.\n",
    "\n",
    "Classification metrics: \\\n",
    "Print the accuracy measure on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbf383b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "110/110 [==============================] - 23s 161ms/step - loss: 2.2350 - accuracy: 0.2120\n",
      "Epoch 2/10\n",
      "110/110 [==============================] - 17s 156ms/step - loss: 2.1563 - accuracy: 0.2210\n",
      "Epoch 3/10\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 2.0102 - accuracy: 0.2750\n",
      "Epoch 4/10\n",
      "110/110 [==============================] - 18s 159ms/step - loss: 1.8646 - accuracy: 0.3298\n",
      "Epoch 5/10\n",
      "110/110 [==============================] - 17s 156ms/step - loss: 1.7327 - accuracy: 0.3723\n",
      "Epoch 6/10\n",
      "110/110 [==============================] - 16s 142ms/step - loss: 1.5912 - accuracy: 0.4185\n",
      "Epoch 7/10\n",
      "110/110 [==============================] - 16s 145ms/step - loss: 1.4240 - accuracy: 0.4871\n",
      "Epoch 8/10\n",
      "110/110 [==============================] - 18s 166ms/step - loss: 1.2436 - accuracy: 0.5628\n",
      "Epoch 9/10\n",
      "110/110 [==============================] - 18s 167ms/step - loss: 1.0644 - accuracy: 0.6395\n",
      "Epoch 10/10\n",
      "110/110 [==============================] - 18s 166ms/step - loss: 0.9255 - accuracy: 0.6957\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18dcf1d05b0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=10, batch_size = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88a0e381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "875/875 [==============================] - 23s 23ms/step - loss: 0.7880 - accuracy: 0.7539\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "742451ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 7s 20ms/step - loss: 2.9080 - accuracy: 0.2480\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fed1e26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
